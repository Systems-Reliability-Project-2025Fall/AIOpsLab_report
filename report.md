# Evaluating LLM Agents for Debugging Cloud (Kubernetes) Issues

## Introduction 

We aim to develop an LLM-driven agent capable of automatically detecting, localizing, analyzing, and mitigating faults in Kubernetes (K8s) clusters.

Today, Kubernetes operations depend heavily on metrics, logs, alerting systems, and human-engineered rules. Debugging typically requires an SRE to manually piece together signals and apply experience-driven heuristics to identify the root cause of failures.

Large Language Models, however, implicitly encode much of this troubleshooting knowledge. Their training data includes countless examples of real-world K8s debugging scenarios, allowing them to internalize the heuristics and reasoning patterns used by human operators. With recent advances in reasoning-capable LLMs, these models increasingly resemble the structured diagnostic processes of experienced engineers.

Although practitioners have begun experimenting with LLMs as assistants within the debugging loop, it remains unclear how well LLMs perform autonomously. Kubernetes provides an ideal testbed for evaluating this: the debugging process is complex, multi-step, and grounded in a standardized API surface that an agent can reliably interact with.

This project explores how effectively LLM agents can handle end-to-end Kubernetes fault diagnosis and remediation, and how their performance compares to human-driven workflows. Our work is based on a open source research project AIOpsLab.

## Background and Related Work

### Kubernetes Reliability and Debugging Practices

Modern cloud applications rely heavily on Kubernetes as the orchestration layer for deploying and scaling containerized workloads. While Kubernetes provides strong abstractions for workload management, its distributed design introduces a wide range of potential failure modes: misconfigured deployments, crashed containers, pod scheduling issues, network partitioning, storage inconsistencies, resource exhaustion, and control-plane instability, among others.

Traditional approaches to Kubernetes troubleshooting center on logs, metrics, and event streams generated by components such as the kubelet, API server, scheduler, and CNI/CSI plugins. SREs navigate this information using tools like `kubectl`, Prometheus, Grafana, and distributed tracing systems. Debugging typically involves iterative testing of hypotheses and cross-reference of signals—an inherently heuristic-heavy process that requires expertise accumulated over time.

### AIOps and Automated Fault Management

The broader field of AIOps seeks to automate aspects of operations management—including anomaly detection, root-cause analysis, forecasting, and remediation—through machine learning. Prior AIOps work focuses heavily on classical statistical techniques, clustering, log template extraction, pattern mining, and anomaly detectors trained on historical telemetry.
While effective for narrow tasks, these methods struggle with generalization and require substantial feature engineering. Recent research introduces graph-based and causality-driven approaches for cloud system diagnosis, yet these systems typically target specific domain signals and do not generalize across the full Kubernetes stack.

AIOpsLab, the open-source project on which this work builds, provides a foundation for evaluating automated operations systems. It offers simulated fault injection, standardized telemetry pipelines, and reproducible debugging scenarios—making it well-suited for benchmarking intelligent agents operating in cloud environments.

### LLMs in System Operations and Infrastructure Management

Large Language Models have recently gained traction as assistants for cloud and DevOps workflows. Industry prototypes and academic studies demonstrate LLMs assisting with tasks such as log summarization, interpreting monitoring dashboards, generating remediation scripts, and providing step-by-step debugging suggestions. 

However, most existing efforts keep a human in the loop. The LLM is treated as a suggestion engine, not an autonomous debugging agent. Studies evaluating LLM performance on infrastructure-related reasoning show potential but also highlight limitations: hallucination, inconsistent grounding in system state, and difficulties executing multi-step operational procedures.

### Multi-Agent and Tool-Use Architectures for Fault Diagnosis

A growing body of research explores LLM agents that interact with external tools—APIs, CLI commands, monitoring systems, and code execution environments. Frameworks such as AutoGPT, ReAct, and various tool-augmented agent architectures demonstrate that LLMs can perform complex tasks when allowed to issue commands, observe system feedback, and revise their internal reasoning.

This line of work is highly relevant to Kubernetes debugging, where effective fault diagnosis requires repeated interaction with cluster APIs: querying pod status, inspecting logs, checking events, and applying configuration fixes. Yet very few studies evaluate the performance of LLM agents in realistic cloud-native environments with real fault injection and ground-truth labels.


## Design – A clear description of your proposed system, technique, or approach.
## Implementation – Details of your technical implementation, including the specific tasks completed by each team member.
## Evaluation – An assessment of how well your system or technique performs based on your chosen metrics or methodology.

